# `replay-benchmark`

This benchmarks the LF engine using transactions from a ledger entries stored in a file.

## Build

Build the tool with Bazel:

    bazel build //daml-lf/snapshot:replay-benchmark

## Running the tool

A ledger entries file can be generated by configuring the engine with
a snapshot directory and submitting commands via a participant's ledger API.

Run the tool using Bazel and pass the benchmark parameters using `-p`
jmh command line functionality:

     bazel run //daml-lf/snapshot:replay-benchmark --   \
      -p entriesFile=<entries file>                    \
      -p choiceName=<exercise choice name>             \
      [-p choiceIndex=<index of the exercise node>]     \
      [-p darFile=<dar file>]                                              


where:

* `<entries file>`: is the full path of the ledger entries
  file to be used separated by commas (`,`).

* `<exercise choice names>`: is the full qualified choice name of the
  root exercise node to be benchmarked separated by commas (`,`). A full
  qualified choice name should be of the form
  `ModuleName:TemplateName:ChoiceName`.  Note the package ID is
  omitted. By default, the tool benchmarks the first choice with
  such a name it finds in the ledger export.

* the optional parameter `<position of the exercise node>` is the
  index of the exercise among the root exercise nodes that matches
  choice name specified by the `choiceName` parameter in the order
  they appear in the export.

* the optional parameter `<dar files>` specify the full path of
  the dar files to be used  separated by commas (`,`). If defined
  the program contained in the dar file is used instead of one
  present in the ledger export, and the export is "adapted" to this
  program. The adaptation process attempts to map the identifiers
  from the export file with the ones of dar file when those latter
  differ only in their package ID.  This can be used when the original
  Daml source used to generate the ledger export is only slightly
  modified or compiled with different options.

## Example Usage

A sample ledger entries file can be generated by running:

     DAR_FILE=$SAVED/ReplayBenchmark-v2.dar \
     SCRIPT_NAME=GenerateSnapshot:globalCreateAndExercise \
     SNAPSHOT_DIR=/tmp/canton/snapshot \
     bazel run --sandbox_debug //daml-lf/snapshot:generate-snapshots

This will save snapshotting information (i.e. the ledger entries file) to a
bazel based temporary file named `$SNAPSHOT_DIR/ReplayBenchmark-v2.dar/globalCreateAndExercise/snapshot-participant.bin`.

Note: here we assume that a `bazel build //daml-lf/snapshot/...` has already been ran to generate the `ReplayBenchmark-v2.dar` Dar file.

A choice may now be benchmarked by JMH using:

    bazel run //daml-lf/snapshot:replay-benchmark -- \
      -p entriesFile=$SNAPSHOT_DIR/ReplayBenchmark-v2.dar/globalCreateAndExercise/snapshot-participant.bin \
      -p choiceName=$CHOICE_NAME \
      -p darFile=$DAR_FILE

Which should generate console output similar to:
```text
# JMH version: 1.36
# VM version: JDK 17.0.10, OpenJDK 64-Bit Server VM, 17.0.10+7-LTS
# VM invoker: /nix/store/c4spavgjc3v6h1yxl7h61gj11lpfb5dw-zulu-ca-jdk-17.0.10/zulu-17.jdk/Contents/Home/bin/java
# VM options: <none>
# Blackhole mode: compiler (auto-detected, use -Djmh.blackhole.autoDetect=false to disable)
# Warmup: 5 iterations, 10 s each
# Measurement: 5 iterations, 10 s each
# Timeout: 10 min per iteration
# Threads: 1 thread, will synchronize iterations
# Benchmark mode: Average time, time/op
# Benchmark: com.digitalasset.daml.lf.testing.snapshot.ReplayBenchmark.bench
# Parameters: (choiceIndex = 0, choiceName = ReplayBenchmark:T:Add, darFile = $SAVED/ReplayBenchmark-v2.dar, entriesFile = $SAVED/snapshot-participant.bin)

# Run progress: 0.00% complete, ETA 00:08:20
# Fork: 1 of 5
# Warmup Iteration   1: %%% loading submission entries from $SAVED/snapshot-participant.bin...
%%% loading dar file $SAVED/ReplayBenchmark-v2.dar ...
SLF4J: No SLF4J providers were found.
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See https://www.slf4j.org/codes.html#noProviders for further details.
%%% compile 30 packages ...
0.033 ms/op
# Warmup Iteration   2: 0.027 ms/op
# Warmup Iteration   3: 0.027 ms/op
# Warmup Iteration   4: 0.027 ms/op
# Warmup Iteration   5: 0.027 ms/op
Iteration   1: 0.027 ms/op
Iteration   2: 0.027 ms/op
Iteration   3: 0.027 ms/op
Iteration   4: 0.027 ms/op
Iteration   5: 0.027 ms/op

# Run progress: 20.00% complete, ETA 00:06:48
# Fork: 2 of 5
# Warmup Iteration   1: %%% loading submission entries from $SAVED/snapshot-participant.bin...
%%% loading dar file $SAVED/ReplayBenchmark-v2.dar ...
SLF4J: No SLF4J providers were found.
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See https://www.slf4j.org/codes.html#noProviders for further details.
%%% compile 30 packages ...
0.030 ms/op
# Warmup Iteration   2: 0.028 ms/op
# Warmup Iteration   3: 0.028 ms/op
# Warmup Iteration   4: 0.028 ms/op
# Warmup Iteration   5: 0.028 ms/op
Iteration   1: 0.028 ms/op
Iteration   2: 0.028 ms/op
Iteration   3: 0.028 ms/op
Iteration   4: 0.028 ms/op
Iteration   5: 0.028 ms/op

# Run progress: 40.00% complete, ETA 00:05:06
# Fork: 3 of 5
# Warmup Iteration   1: %%% loading submission entries from $SAVED/snapshot-participant.bin...
%%% loading dar file $SAVED/ReplayBenchmark-v2.dar ...
SLF4J: No SLF4J providers were found.
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See https://www.slf4j.org/codes.html#noProviders for further details.
%%% compile 30 packages ...
0.030 ms/op
# Warmup Iteration   2: 0.027 ms/op
# Warmup Iteration   3: 0.028 ms/op
# Warmup Iteration   4: 0.027 ms/op
# Warmup Iteration   5: 0.027 ms/op
Iteration   1: 0.027 ms/op
Iteration   2: 0.027 ms/op
Iteration   3: 0.027 ms/op
Iteration   4: 0.027 ms/op
Iteration   5: 0.028 ms/op

# Run progress: 60.00% complete, ETA 00:03:24
# Fork: 4 of 5
# Warmup Iteration   1: %%% loading submission entries from $SAVED/snapshot-participant.bin...
%%% loading dar file $SAVED/ReplayBenchmark-v2.dar ...
SLF4J: No SLF4J providers were found.
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See https://www.slf4j.org/codes.html#noProviders for further details.
%%% compile 30 packages ...
0.030 ms/op
# Warmup Iteration   2: 0.028 ms/op
# Warmup Iteration   3: 0.028 ms/op
# Warmup Iteration   4: 0.028 ms/op
# Warmup Iteration   5: 0.028 ms/op
Iteration   1: 0.028 ms/op
Iteration   2: 0.028 ms/op
Iteration   3: 0.028 ms/op
Iteration   4: 0.028 ms/op
Iteration   5: 0.028 ms/op

# Run progress: 80.00% complete, ETA 00:01:42
# Fork: 5 of 5
# Warmup Iteration   1: %%% loading submission entries from $SAVED/snapshot-participant.bin...
%%% loading dar file $SAVED/ReplayBenchmark-v2.dar ...
SLF4J: No SLF4J providers were found.
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See https://www.slf4j.org/codes.html#noProviders for further details.
%%% compile 30 packages ...
0.030 ms/op
# Warmup Iteration   2: 0.028 ms/op
# Warmup Iteration   3: 0.028 ms/op
# Warmup Iteration   4: 0.028 ms/op
# Warmup Iteration   5: 0.028 ms/op
Iteration   1: 0.028 ms/op
Iteration   2: 0.028 ms/op
Iteration   3: 0.028 ms/op
Iteration   4: 0.028 ms/op
Iteration   5: 0.028 ms/op


Result "com.digitalasset.daml.lf.testing.snapshot.ReplayBenchmark.bench":
  0.028 ±(99.9%) 0.001 ms/op [Average]
  (min, avg, max) = (0.027, 0.028, 0.028), stdev = 0.001
  CI (99.9%): [0.028, 0.028] (assumes normal distribution)


# Run complete. Total time: 00:08:30

REMEMBER: The numbers below are just data. To gain reusable insights, you need to follow up on
why the numbers are the way they are. Use profilers (see -prof, -lprof), design factorial
experiments, perform baseline and negative tests that provide experimental control, make sure
the benchmarking environment is safe on JVM/OS/HW level, ask for reviews from the domain experts.
Do not assume the numbers tell you what you want them to tell.

NOTE: Current JVM experimentally supports Compiler Blackholes, and they are in use. Please exercise
extra caution when trusting the results, look into the generated code to check the benchmark still
works, and factor in a small probability of new VM bugs. Additionally, while comparisons between
different JVMs are already problematic, the performance difference caused by different Blackhole
modes can be very significant. Please make sure you use the consistent Blackhole mode for comparisons.

Benchmark              (choiceIndex)           (choiceName)                      (darFile)                    (entriesFile)  Mode  Cnt  Score    Error  Units
ReplayBenchmark.bench              0  ReplayBenchmark:T:Add  $SAVED/ReplayBenchmark-v2.dar  $SAVED/snapshot-participant.bin  avgt   25  0.028 ±  0.001  ms/op
```

# `replay-profile`

This profiles the replay of a transaction built from a ledger entries file.

     bazel run //daml-lf/snapshot:replay-profile --     \
      --entries <entries files>                         \
      [--dar <dar files>]                               \
      --profile-dir <profile directory>                 \
      --choice <exercise choice names>                  \
      [--exercise-index <exercise index>]


where:

* `<entries files>`: is the full path of the ledger entries
  files to be used separated by commas (`,`)

* `<exercise choice names>`: is the full qualified choice name of the
  root exercise node to be benchmarked separated by commas (`,`). A full
  qualified choice name should be of the form
  `ModuleName:TemplateName:ChoiceName`.  Note the package ID is
  omitted. By default, the tool benchmarks the first choice with
  such a name it finds in the ledger export.

* the optional parameter `<position of the exercise node>` is the
  index of the exercise among the root exercise nodes that matches
  choice name specified by the `choiceName` parameter in the order
  they appear in the export.

* the optional parameter `<dar files>` specify the full path of
  the dar files to be used  separated by commas (`,`). If defined
  the program contained in the dar file is used instead of one
  present in the ledger export, and the export is "adapted" to this
  program. The adaptation process attempts to map the identifiers
  from the export file with the ones of dar file when those latter
  differ only in their package ID.  This can be used when the original
  Daml source used to generate the ledger export is only slightly
  modified or compiled with different options.

* `<profile directory>` is the directory where the profiling output
  will be written.

