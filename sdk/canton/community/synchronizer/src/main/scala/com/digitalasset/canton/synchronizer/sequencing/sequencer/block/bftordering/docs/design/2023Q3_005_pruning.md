# \[Design - Canton BFT ordering service\] Pruning and Persistence

## Overview

Given that the BFT ordering service does not need to store the full history of
sequenced requests in the totally-ordered ledger, it is both advantageous
and necessary (e.g., to satisfy GDPR requirements) to periodically prune the
log of unneeded requests (and more generally, unneeded state).

The ideal pruning strategy is proactive, automated, decentralized, and
(of course) correct in the presence of up to `f` malicious peers. Moreover,
the pruning strategy should respect network governance, correctly supporting
topology changes that add/remove ordering node peers or reconfigure dynamic
network-wide parameters. Finally, the pruning strategy should support the
application (clients) that use the ordering service.

The goal of this document is to identify the durable BFT ordering service state
that is affected by pruning, and to propose several different pruning
strategies that can be implemented over several iterations. Each iteration
should improve the pruning status toward the ideal end state: proactive,
automatic, and decentralized.

## BFT Ordering Service Persistent State

The BFT ordering layer protocol is split across several modules. Each module,
as well as the overarching ordering node itself, may store certain state
that is affected by pruning. Below is the current state estimation.

### [Future] Mempool: Queued Requests

Whether we should persist queued requests in the mempool is still an open question.
There are advantages, in terms of improved client experiences, if the queue is persisted
across node restarts (e.g., after a crash or planned maintenance). However, this comes at
the cost of additional complexity.

Regardless of what we decide, persisting the mempool will likely not be in scope for
the first deliverable.

### Availability Module: Proofs of Availability (PoA)

PoAs must be persisted in order for correct nodes that crash/recover to honor their
promise to the network of storing the referenced requests. Each PoA should be marked
with the epoch (of the originator) in which they were introduced to allow for
[storage fairness](2023Q3_001_mempool-and-availability-modules.md#storage-fairness)
and eventual eviction, if necessary.

### Global: Active Dynamic Ordering Layer Parameters

In the latest dynamic parameters design, it is incorrect to simply store and reference
the most recent `SequencerDomainStateX` topology transaction when it comes to dynamic
ordering service parameters. Although these topology transactions have a dedicated payload
for driver-specific configuration, the sequencer layer and topology processing
are not (yet) able to validate the opaque driver-specific payloads. As a result, topology
transactions with invalid driver-specific payloads may pass validation at sequencing time,
but later fail validation in the ordering layer during deferred activation. If activation
fails, the ordering layer will (deterministically) continue to reuse the dynamic
parameters from the most recently valid `SequencerDomainStateX` transaction. Therefore,
the ordering service must persist this most recently valid transaction to correctly
retrieve the active parameters after a crash recovery.

It is unclear yet whether we need the full signed topology transaction or just the
payload. The former provides signatures by the governors that originally issued the
transaction. The latter can safely be applied after retrieving `f+1` matching copies
from peers.

### Ordering: Most Recent End-of-Epoch (ActivateDeferred) Transaction

The most recent `end_of_epoch` transaction is critical, as it provides nodes with
the necessary `end_of_epoch` timestamp. This timestamp is used to query the
relevant topology state from the local topology processor (in the
sequencer) that applies to the next epoch. In addition, the timestamp
is used to derive each validator's first block timestamp in the next epoch.

Note that because ordering nodes use the most recent `end_of_epoch` transaction to
derive the BFT timestamp of their first block in a new epoch, we don't need to
additionally store the most recently completed block (potentially from an older epoch)
if that epoch has been pruned. The
[BFT Timestamp](https://docs.google.com/document/d/1UeMefivDAa4gW7atbLRuDiUyW8L6Vz7fXCcSgIvG64k/edit) design describes
this in more detail.

### Ordering: Partial Consensus Protocol Progress

The ordering service should persist any partial progress achieved in the underlying
consensus protocol (ISS) instances to ensure that nodes can correctly recover
from a crash with consistency. As the ordering module sequences blocks and persists
the block as part of the
[totally-ordered log](#orderingoutput-totally-ordered-log-of-batchesrequests),
the temporary partial progress stored for that block can be removed.

Exact details regarding the content of the partial progress consensus state
is not yet finalized. At a bare minimum, the ordering service nodes must persist
any protocol-related messages (e.g., Proposal, Prepare, Commit) that this
node locally produced and broadcast. Additionally, ordering nodes may want to
persist protocol messages they receive from peers in an effort to quickly complete
blocks that are only missing a few messages, rather than throwing all peers'
messages away and having to re-obtain them from the network. Finally, there
may be other data (e.g., leader selection policies, current blacklists, etc.)
that should be persisted to ensure consistency.

### Ordering/Output: Totally-Ordered Log (of Batches/Requests)

The ordering service must persist the actual output that it produces, i.e., the
totally-ordered log of ordering requests, in order to continue to serve these requests
to clients (sequencers) and peer ordering nodes (for state transfer) even after a
crash recovery. As mentioned above, the entire history of ordered requests will not
be persisted forever, as periodically all requests prior to some timestamp will be
pruned for efficiency and confidentiality reasons.

At a high level, ths above description is sufficient; simply store the output of the
ordering module (ordered requests) indefinitely until portions (prefixes) of the
log are removed by the periodic pruning process. In practice, however, we need to
be more precise in terms of what data gets stored and where it lives. Ultimately,
we need to understand which format of the data (i.e., post-ordering module or
post-output module) we should persist, and how the data is organized in the
underlying database storage schemas (relational database tables).

#### Stored Data Format

There are essentially two choices in terms of data formats to persist:
1. The resulting data from the _ordering module_, i.e., the sequential blocks
    of batched ordering requests agreed upon by 2f+1 ordering nodes in the
    network.
2. The resulting data from the _output module_, which is actually a deterministic
    transformation applied by each correct node to each resulting block.

One key advantage of the _ordering module_ data format is that the data is
cryptographically verifiable (within the ordering service), as any completed block,
along with the `2f+1` signatures that approved that block, can be presented to any peer
ordering node and verified.

Beyond this cryptographically verifiable trait, there is essentially a trade-off
between both data types:
* _Deterministic Transformation_: Persisting the output module data format has
    the advantage that the data is already transformed when reloading state
    after a crash recovery; the data is prepared for delivery and execution
    at the clients. In contrast, persisting the ordering module data requires
    (deterministically) reapplying the data transformations, e.g., timestamp
    shifting to preserve strictly-increasing BFT timestamps.
* _State Transfer Compatibility_: Persisting the ordering module data format is
    advantageous here for traditional state transfer protocols, as typically peers
    want to retrieve ordering module blocks to fill in gaps in the sequence number
    array, after which deterministic post-processing occurs. In contrast, persisting
    the output module data format requires careful thinking to ensure state
    transfer is correct. If for any reason the original ordering module data is
    needed, their might not be a way to "undo" the applied transformations to
    restore the ordering module data.

Given the above discussion, we plan on persisting the ordering module data
format in the initial prototype, but we will continue to weigh and evaluate
all options as we make progress.

#### Storage Schemas

The underlying storage schemas of the ordering service impacts specific pruning
details. The relational database tables should be designed with efficiency
as a core principle. For example, we should only store one copy of ordering requests'
data, even if there are multiple _references_ to the corresponding ordering requests
throughout the ordering service (e.g., mempool, availability, ordering, and output
modules). The latest design for the database tables is
[linked here](2023Q3_006_schemas.md).

Given these storage schemas, pruning state from the database will likely have a
service-wide impact, affecting multiple tables across several modules. To preserve
ordering service performance during periodic pruning, the database tables should
take the impacts of pruning into account, if possible.

Another potentially interesting idea is to consider the viability of sharing database
tables across the sequencer and ordering service, given that they will (at least
initially) be co-located in the same process. For example, once ordered requests
are delivered to the sequencer, the sequencer can potentially start storing the
requests and corresponding data, and the ordering service can only store lightweight
references to those requests. This approach removes the need to have both the
sequencer and ordering service store a copy of the data. Nonetheless, this is just
a potential optimization and may not work for all deployment scenarios, and therefore
is not recommended without further consideration.

### Peer-to-Peer Connectivity State

Assuming that nodes cannot derive the networking endpoints for each peer from the
retrieved topology state directly, the BFT ordering service should additionally
persist the endpoints for each active peer to enable seamless reconnections
following a crash recovery. In the future, we may decide to include
the endpoint information as part of the topology state, e.g., as a separate set
of new topology transactions, such that ordering nodes do not need to manage
this state directly.

## Admin API

Neither the latest design for the normal ordering service API (acting as a sequencer
driver) nor the governance API require the use of a dedicated admin API directly
attached to the ordering service. These APIs (at least currently) utilize the
existing sequencer APIs for forwarding messages to the ordering service. However,
it is possible that pruning introduces the need for a dedicated (and direct) admin
API access, depending on which [iteration](#iterative-phases) is in effect.

## Pruning Rules

When can a correct ordering node safely prune previously ordered batches of requests?

### Basic pruning rule
Depending on the threat model:
* Assuming all peers are correct, a batch `b` is no longer needed when (i) all ordering
  node peers acknowledge they have received the block containing `b`, and (ii) the
  locally connected sequencer acknowledges it has received all requests contained
  in `b`, i.e., its highest contiguously received request has a sequencer number (or
  timestamp) `>=` the last request in `b`.
* Assuming up to `f` peers are malicious, a batch `b` is no longer needed when (i)
  at least `2f+1` ordering node peers acknowledge they have received the block
  containing `b`, and (ii) the locally connected sequencer acknowledges it has
  received all requests contained in `b`, i.e., its highest contiguously received
  request has a sequencer number (or timestamp) `>=` the last request in `b`

In practice, we cannot assume that all ordering nodes are correct. However, simply
pruning whenever a correct node receives `2f+1` acknowledgements can result in correct,
but slow, nodes that fall behind never receiving some batches. This is because
within the asynchronous model, it is impossible to know if the `f` slowest peers that
have yet to acknowledge are (a) actively malicious and staying quiet, or (b) correct
but experiencing some transient network issue. If the peers are simply slow, they
will be unable to state transfer the (now pruned) missing batches from the
correct peers.

In traditional BFT replication systems, pruning is possible alongside a state
checkpointing strategy, where correct nodes work with the connected application to
create a cumulative state as a result of executing all requests, up to that point in
time. Any replica that misses individual updates can rely on correct replicas
transferring the latest stable state checkpoint, which ensures that the replica
(and its connected application) catches up consistently.

Unfortunately, we cannot employ a state checkpointing strategy because there is no
connected application to provide cumulative state; we are simply an ordering service
providing a totally-ordered stream of requests to connected clients. Therefore, to
successfully prune batches of requests in the presence of `f` faulty nodes, we must
coordinate with the Canton domain sequencers to ensure that the ultimate end users
(the participants) receive all addressed requests.

### Additional pruning considerations
* A correct node does not need to worry about the participants (clients)
  connected to other remote sequencers; the sequencers keep track of the highest
  acknowledged request from each connected domain member (participant, mediator,
  and even other sequencers), and thus the sequencers (not the ordering service) are responsible
  for properly maintaining data for slow participants.
* Prune requests using epoch granularity, i.e., prune all blocks of request batches
  an entire epoch at a time; in particular, do not prune any blocks of request batches
  from the currently active epoch; this ensures that the node can help any straggling
  peers finish the current epoch via state transfer, if necessary.
* Do not prune batches that are unexpired, where expiration is a function over the
  contained requests' `max_sequencing_times`; even if all peer ordering nodes and
  the local sequencer acknowledge they have received the batch, if the batch
  have yet to expire, it is **not** safe to prune because the ordering module must still
  be able to fetch batches based on proofs of availability that are proposed in blocks.
  * Consider the following example: a proof of availability is ordered, delivered to
    the local sequencer, and acknowledged by all peers. Soon after, the proof is
    pruned, along with the rest of the completed epoch. However, the proof has not
    yet expired, according to the batch's `max_sequencing_time`. Next, a malicious peer
    re-submits the same proof of availability in its next proposal. Then, correct
    ordering nodes would order a proof and be unable to retrieve the associated
    request data referenced by that proof.
  * This attack is similar in nature to the
    [garbage collection](2023Q3_001_mempool-and-availability-modules.md#garbage-collection)
    consideration, described in the mempool and availability module design.
  * Keeping unexpired proofs also allows to deduplicate unexpired batches. In particular,
    we can opt to only maintain sufficient metadata (e.g., batch hash or header) for
    the ordering module to deduplicate unexpired batches, while still removing the
    actual data associated with requests from the output log. If we consider that
    clients interact with only the output log, this type of partial pruning is
    effectively "removing" requests from the perspective of clients, up until the
    service itself can really prune all references to a batch altogether. Of course,
    we'll need to carefully design the storage schemas to support this functionality.

Note that while the ordering module must deduplicate proofs of availability
to ensure global liveness, the availability module does not share a similar burden
at the ordering request level: the same individual request could appear in multiple
batches due to participants submitting to multiple sequencers to avoid censorship,
slightly reducing dissemination and ordering bandwidth for unique requests, but
does not impact protocol liveness. In fact, it also does not impact safety, since
the sequencers are already deduplicating requests when reading from the
ordering service. Therefore, providing ordering requests deduplication in the
ordering service directly is really just an optimization.

## Iterative Phases

The goal is to implement pruning using an iterative approach. The first prototype
will employ a simple, but effective, pruning strategy. Subsequent versions of the
ordering service will extend and enhance the pruning feature to be more automated
and decentralized. Below is the plan for pruning iterations:

1. Manual Pruning
   1. Ordering node operators are responsible for pruning their own nodes
   2. The ordering service is not intelligent when it comes to pruning for this first
        phase; operators are responsible for choosing timestamps that are safe to
        prune; it is possible for node operators to shoot themselves in the foot.
   3. Pruning will use the dedicated ordering service [Admin API](#admin-api) to
        provide a dedicated endpoint, implemented as a gRPC service, through which node
        operators will interact.
   4. Node operators may consider using simple automation, e.g., cron job, but again
        care must to taken to avoid pruning unsafe timestamps.
2. Automatic, Distributed, Trustful Pruning
   1. The ordering service is aware of pruning and the associated requirements. Ordering
        nodes periodically exchange information, such as the highest consecutively
        completed block and epoch. Once **_all_** ordering nodes have delivered
        a block to their respective sequencers (and the parent epoch is complete),
        that block is eligible for pruning (barring expiration time considerations).
   2. This pruning strategy assumes that all ordering nodes are honest in terms of
        reporting accurate pruning information. A single malicious node that refuses
        to acknowledge block/epoch progress can block the automatic protocol from
        pruning state.
   3. The manual pruning approach will still be supported as a fallback option in
        the event that some nodes are preventing automatic pruning. Moreover,
        scrutiny should be applied to the node (or set of nodes) that fail to report
        block/epoch progress, potentially prompting repair or network removal.
3. Automatic Decentralized Pruning
   1. The ordering service is capable of automatic pruning, even in the presence
        of up to `f` malicious ordering node peers, by collecting acknowledgements
        from at least a quorum (`2f+1` or `N-f`) ordering nodes.
   2. Despite the ability to tolerate faulty nodes, it is still possible for up
        to `f` correct, but slow, nodes to fall behind. If the fast `f+1` correct
        nodes (and `f` faulty nodes) prune state, these `f` slow nodes may not be
        able to catch up via normal state transfer. As discussed above in the
        [pruning rules](#basic-pruning-rule) section, addressing this scenario
        requires coordination across the domain members to ensure the end users,
        i.e., participants and mediators, receive all relevant ordered requests.
